# VA Signals Hosting + Auth Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Stand up an internal-only, IAP-protected Cloud Run deployment at `va-signals.vetclaims.ai` with Cloud SQL Postgres, scheduled pipelines, and Workspace group access.

**Architecture:** Cloud Run service + IAP + Cloud SQL Postgres; Cloud Run Jobs triggered by Cloud Scheduler; Secret Manager for API keys; Cloud Logging for observability.

**Tech Stack:** Python/FastAPI, Cloud Run, Cloud SQL (Postgres), Cloud Scheduler, Secret Manager, IAP, Cloud Logging.

---

### Task 1: Create the GCP project and enable APIs

**Files:**
- None

**Step 1: Create a new project**
- Run: `gcloud projects create vetclaims-va-signals --organization=<ORG_ID>`
- Expected: project appears in `gcloud projects list`

**Step 2: Link billing**
- Run: `gcloud beta billing projects link vetclaims-va-signals --billing-account=<BILLING_ID>`
- Expected: `gcloud beta billing projects describe vetclaims-va-signals` shows billing enabled

**Step 3: Enable required APIs**
- Run: `gcloud services enable run.googleapis.com sqladmin.googleapis.com secretmanager.googleapis.com iap.googleapis.com cloudscheduler.googleapis.com artifactregistry.googleapis.com`
- Expected: `gcloud services list --enabled` shows all APIs

**Step 4: Set defaults**
- Run: `gcloud config set project vetclaims-va-signals`
- Expected: `gcloud config get-value project` returns `vetclaims-va-signals`

---

### Task 2: Create service accounts and IAM bindings

**Files:**
- None

**Step 1: Create Cloud Run runtime service account**
- Run: `gcloud iam service-accounts create va-signals-runtime`
- Expected: service account exists in IAM

**Step 2: Grant runtime permissions**
- Run:
  - `gcloud projects add-iam-policy-binding vetclaims-va-signals --member="serviceAccount:va-signals-runtime@vetclaims-va-signals.iam.gserviceaccount.com" --role="roles/cloudsql.client"`
  - `gcloud projects add-iam-policy-binding vetclaims-va-signals --member="serviceAccount:va-signals-runtime@vetclaims-va-signals.iam.gserviceaccount.com" --role="roles/secretmanager.secretAccessor"`
- Expected: IAM policy includes both roles

**Step 3: Create scheduler invoker service account**
- Run: `gcloud iam service-accounts create va-signals-scheduler`
- Expected: service account exists in IAM

**Step 4: Grant scheduler invoker roles**
- Run:
  - `gcloud projects add-iam-policy-binding vetclaims-va-signals --member="serviceAccount:va-signals-scheduler@vetclaims-va-signals.iam.gserviceaccount.com" --role="roles/run.invoker"`
- Expected: IAM policy includes run.invoker

---

### Task 3: Provision Cloud SQL Postgres and base schema

**Files:**
- Create: `schema.postgres.sql`
- Modify: `src/db.py`
- Test: `tests/test_db_backend.py`

**Step 1: Create Cloud SQL instance**
- Run: `gcloud sql instances create va-signals-db --database-version=POSTGRES_15 --region=us-central1 --cpu=1 --memory=4GB`
- Expected: instance in `gcloud sql instances list`

**Step 2: Create database and user**
- Run:
  - `gcloud sql databases create va_signals --instance=va-signals-db`
  - `gcloud sql users create va_signals_app --instance=va-signals-db --password=<STRONG_PASSWORD>`
- Expected: database and user exist

**Step 3: Add Postgres schema file**
- Create `schema.postgres.sql` by translating `schema.sql`:
  - Replace SQLite-only syntax (`INTEGER PRIMARY KEY`, `AUTOINCREMENT`, `INSERT OR IGNORE`)
  - Use `SERIAL`/`BIGSERIAL` or `GENERATED BY DEFAULT AS IDENTITY`
  - Use `TEXT`/`JSONB` as appropriate

**Step 4: Add backend-aware schema init**
- Update `src/db.py` so `init_db()` loads `schema.sql` for SQLite and `schema.postgres.sql` for Postgres.

**Step 5: Add backend selection test**
- Add `tests/test_db_backend.py` to assert `DATABASE_URL` selects the Postgres schema path.

**Step 6: Run tests**
- Run: `make test`
- Expected: PASS

---

### Task 4: Make DB access Postgres-compatible

**Files:**
- Modify: `src/db.py`
- Modify: `src/oversight/db_helpers.py`, `src/state/db_helpers.py`, `src/dashboard_api.py` (and any DB helper modules)
- Test: `tests/test_db_backend.py`

**Step 1: Audit SQLite-only SQL**
- Run: `rg "INSERT OR IGNORE|sqlite_master|lastrowid" src`
- Expected: list of SQLite-only patterns to update

**Step 2: Replace SQLite-only SQL**
- Convert `INSERT OR IGNORE` → `INSERT ... ON CONFLICT DO NOTHING`
- Replace `sqlite_master` checks with `information_schema.tables` or `to_regclass`
- Replace `lastrowid` usage with `RETURNING id` queries

**Step 3: Normalize parameter style**
- Standardize on named parameters (e.g., `:doc_id`) via a small helper in `src/db.py`
- Update DB helpers to use the helper for both SQLite and Postgres

**Step 4: Add tests for helper usage**
- Extend `tests/test_db_backend.py` to cover parameter mapping and a simple insert/select round-trip using SQLite

**Step 5: Run tests**
- Run: `make test`
- Expected: PASS

---

### Task 5: Add secrets and runtime configuration

**Files:**
- Modify: `src/notify_slack.py`, `src/fetch_transcripts.py`, `src/summarize.py` (and any secret readers)
- Modify: `README.md` or `docs/ops/runbook.md`
- Test: `tests/test_env_config.py`

**Step 1: Standardize secret lookup**
- Add a `get_secret_env()` helper to read from environment variables first, fallback to keychain locally.

**Step 2: Update secret usage**
- Ensure Slack, Congress API, Anthropic key read from env in Cloud Run.

**Step 3: Add tests**
- Add tests that verify env-first behavior.

**Step 4: Run tests**
- Run: `make test`
- Expected: PASS

---

### Task 6: Create migration script (SQLite → Postgres)

**Files:**
- Create: `scripts/migrate_sqlite_to_postgres.py`
- Modify: `README.md` or `docs/ops/runbook.md`
- Test: `tests/test_migration_script.py`

**Step 1: Write a dry-run export**
- Script should read from SQLite and print row counts per table.

**Step 2: Add Postgres import path**
- Use `psycopg` to insert rows with `ON CONFLICT DO NOTHING`.

**Step 3: Add migration tests**
- Use a temporary SQLite db and validate export counts.

**Step 4: Run tests**
- Run: `make test`
- Expected: PASS

---

### Task 7: Containerize the app for Cloud Run

**Files:**
- Create: `Dockerfile`
- Create: `.dockerignore`
- Modify: `README.md`

**Step 1: Add Dockerfile**
- Use `python:3.11-slim`, install requirements, run `uvicorn` on `$PORT`.

**Step 2: Add .dockerignore**
- Ignore `.venv`, `data/`, `outputs/`, `__pycache__`, local logs.

**Step 3: Document local Docker run**
- Add commands to README for local container test.

**Step 4: Build and run locally**
- Run: `docker build -t va-signals .`
- Run: `docker run -p 8000:8080 -e DATABASE_URL=... va-signals`
- Expected: dashboard loads on localhost

---

### Task 8: Deploy Cloud Run service + IAP + domain

**Files:**
- None

**Step 1: Build and deploy to Cloud Run**
- Run: `gcloud run deploy va-signals --source . --region us-central1 --service-account va-signals-runtime@vetclaims-va-signals.iam.gserviceaccount.com --add-cloudsql-instances <INSTANCE_CONNECTION_NAME> --set-env-vars DATABASE_URL=...`
- Expected: service URL returned

**Step 2: Enable IAP**
- Configure IAP for Cloud Run service in Cloud Console.
- Grant `IAP-secured Web App User` to `leadership@vetclaims.ai` and `employees@vetclaims.ai`.

**Step 3: Map domain**
- Run: `gcloud run domain-mappings create --service va-signals --domain va-signals.vetclaims.ai`
- Expected: SSL managed certificate provisioned

---

### Task 9: Create Cloud Run Jobs + Scheduler triggers

**Files:**
- Optional: `docs/ops/hosting.md` (runbook)

**Step 1: Create one job per pipeline**
- Run: `gcloud run jobs create fr-delta --image <IMAGE> --command "python" --args "-m","src.run_fr_delta"`
- Repeat for `run_ecfr_delta`, `run_bills`, `run_hearings`, `run_oversight`, `state` jobs.

**Step 2: Create scheduler triggers**
- Use Cloud Scheduler HTTP targets to call each job at 6am/6pm offsets.
- Authenticate with `va-signals-scheduler@...` service account.

**Step 3: Verify**
- Run a job manually: `gcloud run jobs execute fr-delta`
- Expected: SUCCESS in job logs, `source_runs` row in Postgres.

---

### Task 10: Smoke tests and cutover

**Files:**
- Modify: `docs/ops/runbook.md`

**Step 1: API smoke tests**
- Hit `/api/runs`, `/api/runs/stats`, `/api/oversight/stats` via IAP-authenticated browser.
- Expected: JSON responses and non-error status.

**Step 2: Validate scheduled run**
- Confirm scheduler ran at least once and `source_runs` updated.

**Step 3: Update runbook**
- Replace local cron instructions with Cloud Run + Scheduler steps.

